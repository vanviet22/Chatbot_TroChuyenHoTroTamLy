import os
os.environ["HF_HOME"] = r"D:\HuggingFace_Cache"
os.environ["HF_HUB_CACHE"] = r"D:\HuggingFace_Cache\hub"
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# model_id = "Qwen/Qwen2-1.5B-Instruct"
# print(f"üöÄ ƒêang t·∫£i m√¥ h√¨nh {model_id} b√™n llm_utils...")

# # Load tokenizer & model
# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     device_map="cpu",
#     offload_folder="D:/HuggingFace_Cache/offload",  # th∆∞ m·ª•c l∆∞u t·∫°m
#     torch_dtype= torch.float32,
#     trust_remote_code=True,
#     low_cpu_mem_usage=True
# )
# model.eval()

def generate_response(tokenizer, model, user_input: str) -> str:
    system_prompt = """B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh, lu√¥n l·∫Øng nghe v√† th·∫•u hi·ªÉu ng∆∞·ªùi d√πng.
        Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d∆∞·ªõi 5 c√¢u, t·ª± nhi√™n v√† ch√¢n th√†nh.
        Lu√¥n ƒë·ªìng c·∫£m, quan t√¢m v√† t√¥n tr·ªçng c·∫£m x√∫c c·ªßa ng∆∞·ªùi d√πng.
        ƒê∆∞a ra g√≥c nh√¨n t√≠ch c·ª±c ho·∫∑c l·ªùi khuy√™n nh·∫π nh√†ng ƒë·ªÉ h·ªç b√¨nh tƒ©nh h∆°n.
        N·∫øu ng∆∞·ªùi d√πng n√≥i ngo√†i ch·ªß ƒë·ªÅ t√¢m l√Ω/c·∫£m x√∫c, h√£y nh·∫π nh√†ng g·ª£i √Ω quay l·∫°i ch·ªß ƒë·ªÅ ch√≠nh.
        Kh√¥ng l·∫∑p l·∫°i nh·ªØng g√¨ ng∆∞·ªùi d√πng ƒë√£ n√≥i."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    formatted = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([formatted], return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.,
            top_p=0.8,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)
    response = text.strip()
    if response.lower().startswith("assistant:"):
        response = response.split("assistant:", 1)[-1].strip()
    return response
