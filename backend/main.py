import os
os.environ["HF_HOME"] = r"D:\HuggingFace_Cache"
os.environ["HF_HUB_CACHE"] = r"D:\HuggingFace_Cache\hub"

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uuid
from datetime import datetime
import logging

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig 
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from agent.agent import Agent
from caching.Cache_mysql import MySQLCache
import torch 

# ----------------- SETUP -----------------
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ----------------- MODEL LOAD  -----------------
model_id = "Qwen/Qwen2-1.5B-Instruct"

print(f"üöÄ ƒêang t·∫£i m√¥ h√¨nh {model_id}...")

# Load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cpu",       
    torch_dtype=torch.float32,
    trust_remote_code=True
)
model.eval()
# T·∫°o pipeline HuggingFace
pipe_agent = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,      
    temperature=0.2,
    top_p=0.9
)
# B·ªçc pipeline th√†nh LLM cho LangChain
hf_pipeline_agent = HuggingFacePipeline(pipeline=pipe_agent)
llm_agent = ChatHuggingFace(llm=hf_pipeline_agent)

# Kh·ªüi t·∫°o agent
agent = Agent(llm=llm_agent)
agent_executor = agent.create_agent()

mysqlcache = MySQLCache()

# ----------------- MODELS -----------------
class MessageIn(BaseModel):
    message: str
    history: list = []

class MessageOut(BaseModel):
    content: str
    sender: str = "bot"
    timestamp: str

# ----------------- HELPERS -----------------
# H·ª£p l·ªãch s·ª≠ v√† chat
def merge_his_mess(user_mess, his):
    messages = []
    if his:
        context = " ".join([f"[Context tr∆∞·ªõc ƒë√≥] {m['content'].strip()}" for m in his])
        messages.append({"role": "system", "content": context})
        
    messages.append({"role": "user", "content": user_mess.strip()})
    return messages

# S·∫£n sinh c√¢u tr·∫£ l·ªùi
def generate_response(user_input: str) -> str:
    system_prompt = """B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh, lu√¥n l·∫Øng nghe v√† th·∫•u hi·ªÉu ng∆∞·ªùi d√πng.
    Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d∆∞·ªõi 5 c√¢u, t·ª± nhi√™n v√† ch√¢n th√†nh.
    Lu√¥n ƒë·ªìng c·∫£m, quan t√¢m v√† t√¥n tr·ªçng c·∫£m x√∫c c·ªßa ng∆∞·ªùi d√πng.
    ƒê∆∞a ra g√≥c nh√¨n t√≠ch c·ª±c ho·∫∑c l·ªùi khuy√™n nh·∫π nh√†ng ƒë·ªÉ h·ªç b√¨nh tƒ©nh h∆°n.
    N·∫øu ng∆∞·ªùi d√πng n√≥i ngo√†i ch·ªß ƒë·ªÅ t√¢m l√Ω/c·∫£m x√∫c, h√£y nh·∫π nh√†ng g·ª£i √Ω quay l·∫°i ch·ªß ƒë·ªÅ ch√≠nh.
    Kh√¥ng l·∫∑p l·∫°i nh·ªØng g√¨ ng∆∞·ªùi d√πng ƒë√£ n√≥i."""


    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    formatted = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([formatted], return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens= 128,
            temperature=0.3,
            top_p=0.8,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)
    response = text.strip()
    if response.lower().startswith("assistant:"):
        response = response.split("assistant:", 1)[-1].strip()
    return response

# print("c√¢u tr·∫£ l·ªùi:",generate_response("H√¥m nay t√¥i h∆°i bu·ªìn b·∫°n c√≥ th·ªÉ tr√≤ chuy·ªán v·ªõi t√¥i ƒë∆∞·ª£c kh"))
# ----------------- ROUTES -----------------
@app.post("/response", response_model=MessageOut)
async def get_response(message: str):
    try:
        response = generate_response(message)
        return MessageOut(
            content=response,
            sender="bot",
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        logger.exception("L·ªói t·∫°i endpoint /response")
        return MessageOut(
            content="Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi g·ªçi LLM tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.",
            sender="bot",
            timestamp=datetime.now().isoformat()
        )

@app.post("/chatbot", response_model=MessageOut)
async def chat_reply(msg: MessageIn):
    try:
        if resp := mysqlcache.search_with_vectorstore(msg.message):
            logger.info("L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ mysql cache")
            response_text = resp["answer"]
        else:
            messages = merge_his_mess(msg.message, msg.history)
            result = agent_executor.invoke({"input": messages})
            if isinstance(result, dict):
                response_text = result.get("output") or result.get("messages", [])[-1].content
            else:
                response_text = str(result)
        return MessageOut(
            content=response_text,
            sender="bot",
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        logger.exception("L·ªói t·∫°i endpoint /chatbot")
        return MessageOut(
            content="Xin l·ªói, ƒë√£ x·∫£y ra l·ªói endpoint chatbot. Vui l√≤ng th·ª≠ l·∫°i.",
            sender="bot",
            timestamp=datetime.now().isoformat()
        )

# ----------------- RUN -----------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000,reload=False)